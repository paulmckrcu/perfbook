% formal/formal.tex
% mainfile: ../perfbook.tex
% SPDX-License-Identifier: CC-BY-SA-3.0

\QuickQuizChapter{chp:Formal Verification}{Formal Verification}{qqzformal}
%
\Epigraph{Beware of bugs in the above code; I have only proved it correct,
	  not tried it.}{Donald Knuth}

\OriginallyPublished{chp:Formal Verification}{Formal Verification}{Linux Weekly News}{PaulEMcKenney2007QRCUspin,PaulEMcKenney2008dynticksRCU,PaulEMcKenney2011ppcmem}

Parallel algorithms can be hard to write, and even harder to debug.
Testing, though essential, is insufficient, as fatal \IXpl{race condition}
can have extremely low probabilities of occurrence.
Proofs of correctness can be valuable, but in the end are just as
prone to human error as is the original algorithm.
In addition, a proof of correctness cannot be expected to find errors
in your assumptions, shortcomings in the requirements,
misunderstandings of the underlying software or hardware primitives,
or errors that you did not think to construct a proof for.
This means that formal methods can never replace testing.
Nevertheless, formal methods can be a valuable addition to your validation
toolbox.

It would be very helpful to have a tool that could somehow locate
all race conditions.
A number of such tools exist, for example,
\cref{sec:formal:State-Space Search} provides an
introduction to the general-purpose state-space search tools Promela and Spin,
\cref{sec:formal:Special-Purpose State-Space Search}
similarly introduces the special-purpose ppcmem tool,
\cref{sec:formal:Axiomatic Approaches}
looks at an example axiomatic approach,
\cref{sec:formal:SAT Solvers}
briefly overviews SAT solvers,
\cref{sec:formal:Stateless Model Checkers}
briefly overviews stateless model checkers,
\cref{sec:formal:Summary}
sums up use of formal-verification tools for verifying parallel algorithms,
and finally
\cref{sec:formal:Choosing a Validation Plan}
discusses how to decide how much and what type of validation to apply
to a given software project.

\input{formal/spinhint}
\input{formal/dyntickrcu}
\input{formal/ppcmem}
\input{formal/axiomatic}
\input{formal/sat}
\input{formal/stateless}

\section{Summary}
\label{sec:formal:Summary}
%
\epigraph{Western thought has focused on True-False;
	  it is high time to shift to Robust-Fragile.}
	 {Nassim Nicholas Taleb, summarized}
% Full quote:
% Since Plato, Western thought and the theory of knowledge has focused on
% the notions of True-False; as commendable as that was, it is high time
% to shift the concern to Robust-Fragile, and social epistemology to the
% more serious problem of Sucker-Nonsucker.

The formal-verification techniques described in this chapter
are very powerful tools for validating small
parallel algorithms, but they should not be the only tools in your toolbox.
Despite decades of focus on formal verification, testing remains the
validation workhorse for large parallel software
systems~\cite{JonathanCorbet2006lockdep,DaveJones2011Trinity,PaulEMcKenney2016Formal}.

It is nevertheless quite possible that this will not always be the case.
To see this, consider that there is estimated to be more than twenty
billion instances of the Linux kernel as of 2017.
Suppose that the Linux kernel has a bug that manifests on average every million
years of runtime.
As noted at the end of the preceding chapter, this bug will be appearing
more than 50 times \emph{per day} across the installed base.
But the fact remains that most formal validation techniques can be used
only on very small codebases.
So what is a concurrency coder to do?

Think in terms of finding the first bug, the first relevant bug, the
last relevant bug, and the last bug.

The first bug is normally found via inspection or compiler diagnostics.
Although the increasingly sophisticated compiler diagnostics comprise
a lightweight sort of formal verification, it is not common to think of
them in those terms.
This is in part due to an odd practitioner prejudice which says ``If I am
using it, it cannot be formal verification'' on the one hand, and a large
gap between compiler diagnostics and verification research on the other.

Although the first relevant bug might be located via inspection or
compiler diagnostics, it is not unusual for these two steps to find
only typos and false positives.
Either way, the bulk of the relevant bugs, that is, those bugs that
might actually be encountered in production, will often be found via testing.

When testing is driven by anticipated or real use cases, it is not
uncommon for the last relevant bug to be located by testing.
This situation might motivate a complete rejection of formal verification,
however, irrelevant bugs have an annoying habit of suddenly becoming relevant
at the least convenient moment possible, courtesy of black-hat attacks.
For security-critical software, which appears to be a continually
increasing fraction of the total, there can thus be strong motivation
to find and fix the last bug.
Testing is demonstrably unable to find the last bug, so there is a
possible role for formal verification, assuming, that is, that
formal verification proves capable of growing into that role.
As this chapter has shown, current formal verification systems are
extremely limited.

\QuickQuiz{
	But shouldn't sufficiently low-level software be for all intents
	and purposes immune to being exploited by black hats?
}\QuickQuizAnswer{
	Unfortunately, no.

	At one time, Paul E. McKenny felt that Linux-kernel RCU
	was immune to such exploits, but the advent of Row Hammer
	showed him otherwise.
	After all, if the black hats can hit the system's DRAM,
	they can hit any and all low-level software, even including RCU\@.

	And in 2018, this possibility passed from the realm of
	theoretical speculation into the hard and fast realm of
	objective reality~\cite{McKenney:2019:CRS:3319647.3325836}.
}\QuickQuizEnd

Please note that formal verification is often much harder to use than
is testing.
This is in part a cultural statement, and there is reason to hope
that formal verification will be perceived to be easier with increased
familiarity.
That said, very simple test harnesses can find significant bugs in arbitrarily
large software systems.
In contrast, the effort required to apply formal verification seems to
increase dramatically as the system size increases.

I have nevertheless made occasional use of formal verification
for almost 30 years by playing to formal verification's strengths,
namely design-time verification of small complex portions of the overarching
software construct.
The larger overarching software construct is of course validated by testing.

\QuickQuiz{
	In light of the full verification of the L4 microkernel,
	isn't this limited view of formal verification just a little
	bit obsolete?
}\QuickQuizAnswer{
	Unfortunately, no.

	The first full verification of the L4 microkernel was a tour de force,
	with a large number of Ph.D.~students hand-verifying code at a
	very slow per-student rate.
	This level of effort could not be applied to most software projects
	because the rate of change is just too great.
	Furthermore, although the L4 microkernel is a large software
	artifact from the viewpoint of formal verification, it is tiny
	compared to a great number of projects, including LLVM,
	\GCC, the Linux kernel, Hadoop, MongoDB, and a great many others.
	In addition, this verification did have limits, as the researchers
	freely admit, to their credit:
	\url{https://docs.sel4.systems/projects/sel4/frequently-asked-questions.html\#does-sel4-have-zero-bugs}.

	Although formal verification is finally starting to show some
	promise, including more-recent L4 verifications involving greater
	levels of automation, it currently has no chance of completely
	displacing testing in the foreseeable future.
	And although I would dearly love to be proven wrong on this point,
	please note that such proof will be in the form of a real tool
	that verifies real software, not in the form of a large body of
	rousing rhetoric.

	Perhaps someday formal verification will be used heavily for
	validation, including for what is now known as regression testing.
	\Cref{sec:future:Formal Regression Testing?} looks at
	what would be required to make this possibility a reality.
}\QuickQuizEnd

One final approach is to consider the following two definitions from
\cref{sec:debugging:Required Mindset}
and the consequence that they imply:

\begin{description}[itemsep=0pt,labelindent=1em]
\item[Definition:]	Bug-free programs are trivial programs.
\item[Definition:]	Reliable programs have no known bugs.
\item[Consequence:]	Any non-trivial reliable program contains at least
			one as-yet-unknown bug.
\end{description}

From this viewpoint, any advances in validation and verification can
have but two effects:
\begin{enumerate*}[(1)]
\item An increase in the number of trivial programs or
\item A decrease in the number of reliable programs.
\end{enumerate*}
Of course, the human race's increasing reliance on multicore systems and
software provides extreme motivation for a very sharp increase in the
number of trivial programs.

However, if your code is so complex that you find yourself
relying too heavily on formal-verification
tools, you should carefully rethink your design, especially if your
formal-verification tools require your code to be hand-translated
to a special-purpose language.
For example, a complex implementation of the dynticks interface for
preemptible RCU that was presented in
\cref{sec:formal:Promela Parable: dynticks and Preemptible RCU}
turned out to
have a much simpler alternative implementation, as discussed in
\cref{sec:formal:Simplicity Avoids Formal Verification}.
All else being equal, a simpler implementation is much better than
a proof of correctness for a complex implementation.

And the open challenge to those working on formal verification techniques
and systems is to prove this summary wrong!
To assist in this task, Verification Challenge~6 is now
available~\cite{PaulEMcKenney2017VerificationChallenge6}.
Have at it!!!

\section{Choosing a Validation Plan}
\label{sec:formal:Choosing a Validation Plan}
%
\epigraph{Science is a first-rate piece of furniture for one's upper
	  chamber, but only given common sense on the ground floor.}
	 {Oliver Wendell Holmes, updated}

What sort of validation should you use for your project?

As is often the case in software in particular and in engineering
in general, the answer is ``it depends''.

Note that neither running a test nor undertaking formal verification
will change your project.
At best, such effort have an indirect effect by locating a bug that
is later fixed.
Nevertheless, fixing a bug might prevent inconvenience, monetary loss,
property damage, or even loss of life.
Clearly, this sort of indirect effect can be extremely valuable.

Unfortunately, as we have seen, it is difficult to predict whether or
not a given validation effort will find important bugs.
It is therefore all too easy to invest too little---or even to fail
to invest at all, especially if development estimates proved overly
optimistic or budgets unexpectedly tight, conditions which almost
always come into play in real-world software projects.

The decision to nevertheless invest in validation is often forced by
experienced people with forceful personalities.
But this is no guarantee, given that other stakeholders might also
have forceful personalities.
Worse yet, these other stakeholders might bring stories of expensive
validation efforts that nevertheless allowed embarrassing bugs to
escape to the end users.
So although a scarred, grey-haired, and grouchy veteran might carry
the day, a more organized approach would perhaps be more useful.

Fortunately, there is a strictly financial analog to investments in
validation, and that is the insurance policy.

\IfTwoColumn{
\begin{figure*}
\centering
\resizebox{6in}{!}{\includegraphics{CodeSamples/formal/data/RCU-test-ratio.pdf}}
\caption{Linux-Kernel RCU Test Code}
\label{fig:formal:Linux-Kernel RCU Test Code}
\end{figure*}
}{}

Both insurance policies and validation efforts require consistent
up-front investments, and both defend against disasters that might
or might not ever happen.
Furthermore, both have exclusions of various types.
For example, insurance policies for coastal areas might exclude
damages due to tidal waves, while on the other hand we have seen
that there is not yet any validation methodology that can find
each and every bug.

In addition, it is possible to over-invest in both insurance and
in validation.
For but one example, a validation plan that consumed the entire
development budget would be just as pointless as would an insurance
policy that covered the Sun going nova.

One approach is to devote a given fraction of the software budget to
validation, with that fraction depending on the criticality of the
software, so that safety-critical avionics software might grant a
larger fraction of its budget to validation than would a homework
assignment.
Where available, experience from prior similar projects should be
brought to bear.
However, it is necessary to structure the project so that the validation
investment starts when the project does, otherwise the inevitable overruns
in spending on coding will crowd out the validation effort.

Staffing start-up projects with experienced people can result in
overinvestment in validation efforts.
Just as it is possible to go broke buying too much insurance, it is
possible to kill a project by investing too much in testing.
This is especially the case for first-of-a-kind projects where it is
not yet clear which use cases will be important, in which case testing
for all possible use cases will be a possibly fatal waste of time,
energy, and funding.

However, as the tasks supported by a start-up project become more routine,
users often become less forgiving of failures, thus increasing the need
for validation.
Managing this shift in investment can be extremely challenging,
especially in the all-too-common case where the users are unwilling
or unable to disclose the exact nature of their use case.
It then becomes critically important to reverse-engineer the
use cases from bug reports and from discussions with the users.
As these use cases are better understood, use of continuous integration
can help reduce the cost of finding and fixing any bugs located.

\IfTwoColumn{}{
\begin{figure}
\centering
\IfEbookSize{
\resizebox{\onecolumntextwidth}{!}{\includegraphics{CodeSamples/formal/data/RCU-test-ratio-crop.pdf}}
}{
\resizebox{4.5in}{!}{\includegraphics{CodeSamples/formal/data/RCU-test-ratio-crop.pdf}}
}
\caption{Linux-Kernel RCU Test Code}
\label{fig:formal:Linux-Kernel RCU Test Code}
\end{figure}
}

One example evolution of a software project's use of validation is
shown in
\cref{fig:formal:Linux-Kernel RCU Test Code}.
As can be seen in the \lcnamecref{fig:formal:Linux-Kernel RCU Test Code},
Linux-kernel RCU didn't have any validation code whatsoever until Linux
kernel v2.6.15, which was released more than three years after the
October 2002 v2.5.43 release that added RCU to the Linux kernel.
The test suite achieved its peak fraction of the total lines of code
in Linux kernel v2.6.19--v2.6.21.
This fraction decreased sharply with the acceptance of preemptible RCU
for real-time applications in v2.6.25.
This decrease was due to the fact that the RCU API was identical
in the preemptible and non-preemptible variants of RCU\@.
This in turn meant that the existing test suite applied to both variants,
so that even though the Linux-kernel RCU code expanded significantly,
there was no need to expand the tests.

Subsequent bars in \cref{fig:formal:Linux-Kernel RCU Test Code} show
that the RCU code base expanded significantly, but that the
corresponding validation code expanded even more dramatically.
Linux kernel v3.5 added tests for the \co{rcu_barrier()} API, closing
a long-standing hole in test coverage.
Linux kernel v3.14 added automated testing and analysis of test results,
moving RCU towards continuous integration.
Linux kernel v4.7 added a performance validation suite for RCU's update-side
primitives.
Linux kernel v4.12 added Tree SRCU, featuring improved update-side
scalability, and v4.13 removed the old less-scalable SRCU implementation.
Linux kernel v5.0 briefly hosted the \path{nolibc} library within
the rcutorture scripting directory before it moved to its long-term
home in \path{tools/include/nolibc}.
Linux kernel v5.8 added the Tasks Trace and Rude flavors of RCU\@.
Linux kernel v5.9 added the \path{refscale.c} suite of read-side performance
tests.
Linux kernels v5.12 and v5.13 started adding the ability to change a
given CPU's callback-offloading status at runtime and also added the
\path{torture.sh} acceptance-test script.
Linux kernel v5.14 added distributed rcutorture.
Linux kernel v5.15 added demonic vCPU placement in rcutorture testing,
which was successful in locating a number of race conditions.\footnote{
	The trick is to place one pair of vCPUs within the same core on
	one socket, while placing another pair within the same core on
	some other socket.
	As you might expect from \cref{chp:Hardware and its Habits},
	this produces different memory latencies between different
	pairs of vCPUs
	(\url{https://paulmck.livejournal.com/62071.html}).}
Linux kernel v5.17 removed the \co{RCU_FAST_NO_HZ} Kconfig option.
Linux kernel v6.14 moved \co{kvfree_rcu()} into SLAB (in
\path{mm/slab_common.c})~\cite{20241212180208.274813-1-urezki}.
Numerous other changes may be found in the Linux kernel's \co{git} archives.
% rcutorture
% v2.6.15: First torture test
% v2.6.19: SRCU: Plugin architecture avoids test-code explosion.
% v2.6.19-21: Peak test fraction.
% v2.6.25: preemptible RCU, consistent API avoids added test code.
% v3.4: Add tests for RCU CPU stall warnings.
% v3.5: Add tests for rcu_barrier(). *
% v3.14: Add rcutorture scripting automating tests and results analysis. *
% v3.15: Add support for multiple torture-tests suites for locktorture.
% v3.16: Add support for conditional grace-period primitives.
% v4.7: Add update-side performance validation suite. *
% v4.12: Added Tree SRCU.
% v4.13: Removed non-Tree SRCU.
% v5.0: nolibc was briefly in the rcutorture scripting directory.
% v5.8: Added Tasks Trace RCU and Rude RCU.
% v5.9: Added refscale.c.
% v5.12: Added torture.sh.  Added runtime adjustment of rcu_nocbs CPUs.
% v5.13: More rcu_nocbs CPUs, polls grace-period primitives, remote rcutorture.
% v5.14: Added demonic affinity to rcutorture scripting.
% v5.17: Added RCU callback (de)offloading optimizations and per-CPU
%	 tracking of RCU Tasks callbacks.
% v5.18: Improved RCU priority boosting and \co{rcu_barrier()} no longer
%	 blocks CPU-hotplug operations.
% v5.19: Enabled milliseconds-scale real-time response from
%	 \co{synchronize_rcu_expedited()} and also dynamically allocated
%	 and sized Tree SRCU's \co{srcu_node} array.  This last saves
%	 significant memory on kernels sized for thousands of CPUs
%	 that typically run on much smaller systems.

We have established that the validation budget varies from one project
to the next, and also over the lifetime of any given project.
But how should the validation investment be split between testing and
formal verification?

This question is being answered naturally as compilers adopt increasingly
aggressive formal-verification techniques into their diagnostics and
as formal-verification tools continue to mature.
In addition, the Linux-kernel lockdep and KCSAN tools illustrate the
advantages of combining formal verification techniques with run-time
analysis, as discussed in \cref{sec:debugging:Assertions}.
Other combined techniques analyze traces gathered from
executions~\cite{DanielBristot2019RTtrace}.
For the time being, the best practice is to focus first on testing and to
reserve explicit work on formal verification for those portions of the
project that are not well-served by testing, and that have exceptional
needs for robustness.
For example, Linux-kernel RCU relies primarily on testing, but has
made occasional use of formal verification as discussed in this chapter.

In short, choosing a validation plan for concurrent software remains
more an art than a science, let alone a field of engineering.
However, there is every reason to expect that increasingly rigorous
approaches will continue to become more prevalent.

\QuickQuizAnswersChp{qqzformal}
